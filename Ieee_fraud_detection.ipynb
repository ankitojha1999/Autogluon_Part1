{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1/sHnA/sFpitSSRGt3vKQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitojha1999/Autogluon_Part1/blob/main/Ieee_fraud_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv3r-MQY9DFR",
        "outputId": "f6133527-76f8-4aff-9161-9bca532bbd64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at mount; to attempt to forcibly remount, call drive.mount(\"mount\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('mount')\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/mount/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REmaKqMy9IHL",
        "outputId": "2e0af2e6-ffc8-4049-ac79-767456981b97"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/ieee-fraud-detection\n",
        "!mkdir -p /content/ieee-fraud-detection\n",
        "!kaggle competitions download -c ieee-fraud-detection -p /content/ieee-fraud-detection\n",
        "\n",
        "!unzip /content/ieee-fraud-detection/ieee-fraud-detection.zip -d /content/ieee-fraud-detection/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMSPznv_9hLI",
        "outputId": "9a5f53e9-c1dd-4eec-c7dc-049c87cf308f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 7, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 414, in authenticate\n",
            "    self._load_config(config_data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 465, in _load_config\n",
            "    raise ValueError('Error: Missing %s in configuration.' % item)\n",
            "ValueError: Error: Missing username in configuration.\n",
            "unzip:  cannot find or open /content/ieee-fraud-detection/ieee-fraud-detection.zip, /content/ieee-fraud-detection/ieee-fraud-detection.zip.zip or /content/ieee-fraud-detection/ieee-fraud-detection.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autogluon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4rrOQIA9nnf",
        "outputId": "bd889640-5109-43be-8d9b-ad7b8b5c9ed1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: autogluon in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: autogluon.core==1.1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (1.1.1)\n",
            "Requirement already satisfied: autogluon.features==1.1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon) (1.1.1)\n",
            "Requirement already satisfied: autogluon.tabular==1.1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (1.1.1)\n",
            "Requirement already satisfied: autogluon.multimodal==1.1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon) (1.1.1)\n",
            "Requirement already satisfied: autogluon.timeseries==1.1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries[all]==1.1.1->autogluon) (1.1.1)\n",
            "Requirement already satisfied: numpy<1.29,>=1.21 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.13,>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.12.0)\n",
            "Requirement already satisfied: scikit-learn<1.4.1,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.4.0)\n",
            "Requirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.3)\n",
            "Requirement already satisfied: pandas<2.3.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.2.2)\n",
            "Requirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.7.1)\n",
            "Requirement already satisfied: boto3<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.35.34)\n",
            "Requirement already satisfied: autogluon.common==1.1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.1.1)\n",
            "Requirement already satisfied: ray<2.11,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.10.0)\n",
            "Requirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (0.2.7)\n",
            "Requirement already satisfied: Pillow<11,>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (10.4.0)\n",
            "Requirement already satisfied: torch<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.3.1)\n",
            "Requirement already satisfied: lightning<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.3.3)\n",
            "Requirement already satisfied: transformers<4.41.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon) (4.39.3)\n",
            "Requirement already satisfied: accelerate<0.22.0,>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.21.0)\n",
            "Requirement already satisfied: jsonschema<4.22,>=4.18 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (4.21.1)\n",
            "Requirement already satisfied: seqeval<1.3.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (1.2.2)\n",
            "Requirement already satisfied: evaluate<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.4.3)\n",
            "Requirement already satisfied: timm<0.10.0,>=0.9.5 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.9.16)\n",
            "Requirement already satisfied: torchvision<0.19.0,>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.18.1)\n",
            "Requirement already satisfied: scikit-image<0.21.0,>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.20.0)\n",
            "Requirement already satisfied: text-unidecode<1.4,>=1.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (1.3)\n",
            "Requirement already satisfied: torchmetrics<1.3.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (1.2.1)\n",
            "Requirement already satisfied: nptyping<2.5.0,>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.4.1)\n",
            "Requirement already satisfied: omegaconf<2.3.0,>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.2.3)\n",
            "Requirement already satisfied: pytorch-metric-learning<2.4,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.3.0)\n",
            "Requirement already satisfied: nlpaug<1.2.0,>=1.1.10 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (1.1.11)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.4.5 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (3.8.1)\n",
            "Requirement already satisfied: openmim<0.4.0,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.3.9)\n",
            "Requirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.7.1)\n",
            "Requirement already satisfied: jinja2<3.2,>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (3.1.4)\n",
            "Requirement already satisfied: tensorboard<3,>=2.9 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.17.0)\n",
            "Requirement already satisfied: pytesseract<0.3.11,>=0.3.9 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.3.10)\n",
            "Requirement already satisfied: nvidia-ml-py3==7.352.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (7.352.0)\n",
            "Requirement already satisfied: pdf2image<1.19,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (1.17.0)\n",
            "Requirement already satisfied: xgboost<2.1,>=1.6 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (2.0.3)\n",
            "Requirement already satisfied: fastai<2.8,>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (2.7.17)\n",
            "Requirement already satisfied: lightgbm<4.4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (4.3.0)\n",
            "Requirement already satisfied: catboost<1.3,>=1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (1.2.7)\n",
            "Requirement already satisfied: joblib<2,>=1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (1.4.2)\n",
            "Requirement already satisfied: pytorch-lightning<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.3.3)\n",
            "Requirement already satisfied: gluonts==0.15.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.15.1)\n",
            "Requirement already satisfied: statsforecast<1.5,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (1.4.0)\n",
            "Requirement already satisfied: mlforecast<0.10.1,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.10.0)\n",
            "Requirement already satisfied: utilsforecast<0.0.11,>=0.0.10 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.0.10)\n",
            "Requirement already satisfied: orjson~=3.9 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (3.10.7)\n",
            "Requirement already satisfied: optimum<1.19,>=1.17 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.18.1)\n",
            "Requirement already satisfied: psutil<6,>=5.7.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (71.0.4)\n",
            "Requirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.9.2)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (4.12.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon) (6.0.2)\n",
            "Requirement already satisfied: botocore<1.36.0,>=1.35.34 in /usr/local/lib/python3.10/dist-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.35.34)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (0.10.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (0.20.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (1.16.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (3.0.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.24.7)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (24.1.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.8,>=1.5.29 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.7.10)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.3)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.7.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (0.10.9.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.1.1->autogluon) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (0.20.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (0.11.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.60.0)\n",
            "Requirement already satisfied: window-ops in /usr/local/lib/python3.10/dist-packages (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.0.15)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (5.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==1.1.1->autogluon) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==1.1.1->autogluon) (2024.9.11)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.3.0,>=2.1.1->autogluon.multimodal==1.1.1->autogluon) (4.9.3)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.4.6)\n",
            "Requirement already satisfied: model-index in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.1.11)\n",
            "Requirement already satisfied: opendatalab in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.0.10)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (13.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.9.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (15.0.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.13.3)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.17.0)\n",
            "Requirement already satisfied: onnxruntime>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.19.2)\n",
            "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (3.20.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (3.16.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.0.8)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.4.1)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (16.1.0)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (3.10.8)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.7.0)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.5.6)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.3.14)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.11.4)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.21.0)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (7.0.4)\n",
            "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (20.26.6)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.64.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2024.8.30)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (2.35.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (2024.9.20)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (1.7.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.5.0)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from statsforecast<1.5,>=1.4.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.14.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (1.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.0.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm<0.10.0,>=0.9.5->autogluon.multimodal==1.1.1->autogluon) (0.4.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (12.6.68)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<4.41.0,>=4.38.0->transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon) (0.15.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon) (0.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.1.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.4.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (4.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (4.12.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.43.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (24.3.25)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.7->gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.7->gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.23.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.12.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.4.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->statsforecast<1.5,>=1.4.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.5.6)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.3.8)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (4.3.6)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (10.0)\n",
            "Requirement already satisfied: ordered-set in /usr/local/lib/python3.10/dist-packages (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (4.1.0)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.19.2)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (3.21.0)\n",
            "Requirement already satisfied: openxlab in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.0.11)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (9.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.3.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.65.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.24.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.27.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.1.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.19.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (1.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (4.9)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from autogluon.tabular import TabularPredictor\n",
        "\n",
        "directory = '/content/ieee-fraud-detection/'\n",
        "label = 'isFraud'\n",
        "eval_metric = 'roc_auc'\n",
        "save_path = directory + 'auto-gluon-model'\n",
        "\n",
        "train_identity = pd.read_csv(directory + 'train_identity.csv')\n",
        "train_transaction = pd.read_csv(directory + 'train_transaction.csv')\n",
        "print(f\"train_transaction columns: {train_transaction.columns.tolist()}\")\n",
        "print(f\"train_identity columns: {train_identity.columns.tolist()}\")\n"
      ],
      "metadata": {
        "id": "9vhIUgkmvos4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4765b83-2267-4594-cd18-0e63513b4ae5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_transaction columns: ['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']\n",
            "train_identity columns: ['TransactionID', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')"
      ],
      "metadata": {
        "id": "KFeKQTChvr0h"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = TabularPredictor(label=label, eval_metric=eval_metric, path=save_path, verbosity=2).fit(\n",
        "    train_data.sample(n=500), presets='high_quality',time_limit=3600,num_gpus=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC_3jHdCvtLB",
        "outputId": "61871ee1-df24-4767-901c-46998a24fc31"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       3.85 GB / 12.67 GB (30.4%)\n",
            "Disk Space Avail:   61.10 GB / 107.72 GB (56.7%)\n",
            "===================================================\n",
            "Presets specified: ['high_quality']\n",
            "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
            "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
            "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
            "2024-10-06 05:14:51,360\tINFO worker.py:1743 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
            "\t\tContext path: \"/content/ieee-fraud-detection/auto-gluon-model/ds_sub_fit/sub_fit_ho\"\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Running DyStack sub-fit ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Beginning AutoGluon training ... Time limit = 893s\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m AutoGluon will save models to \"/content/ieee-fraud-detection/auto-gluon-model/ds_sub_fit/sub_fit_ho\"\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Train Data Rows:    444\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Train Data Columns: 433\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Label Column:       isFraud\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Problem Type:       binary\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Preprocessing data ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Using Feature Generators to preprocess the data ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tAvailable Memory:                    3526.17 MB\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tTrain Data (Original)  Memory Usage: 1.92 MB (0.1% of available memory)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tStage 1 Generators:\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\t\tNote: Converting 21 features to boolean dtype as they only contain 2 unique values.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tStage 2 Generators:\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tStage 3 Generators:\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tStage 4 Generators:\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tStage 5 Generators:\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tUseless Original Features (Count: 8): ['V107', 'V111', 'V113', 'V117', 'V118', 'V119', 'V120', 'V305']\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\tThis is typically a feature which has the same value for all rows.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tUnused Original Features (Count: 19): ['V16', 'V18', 'V21', 'V22', 'V28', 'V32', 'V42', 'V68', 'V93', 'V114', 'V157', 'V196', 'V199', 'V237', 'V241', 'V244', 'V252', 'V278', 'id_27']\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\t('float', [])  : 18 | ['V16', 'V18', 'V21', 'V22', 'V28', ...]\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\t('object', []) :  1 | ['id_27']\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\t('float', [])  : 373 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\t('int', [])    :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\t('object', []) :  30 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\t('category', [])  :  29 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\t('float', [])     : 357 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\t('int', [])       :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\t('int', ['bool']) :  17 | ['C3', 'M1', 'V1', 'V14', 'V41', ...]\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t3.5s = Fit runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t406 features in original data used to generate 406 features in processed data.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tTrain Data (Processed) Memory Usage: 1.25 MB (0.0% of available memory)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Data preprocessing and feature engineering runtime = 3.54s ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m User-specified model hyperparameters to be fit:\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m {\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m }\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting 110 L1 models ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 592.61s of the 889.11s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.5878\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.08s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.13s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 587.28s of the 883.78s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.5953\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.06s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.07s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 587.05s of the 883.55s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=2.44%)\n",
            "\u001b[36m(_ray_fit pid=31840)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=31840)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=31840)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=31840)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=31840)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=31840)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=31840)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=31919)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=31919)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=31919)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=31919)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=31919)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=31919)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32003)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32003)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32003)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32003)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32003)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32003)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32099)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32099)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32099)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32099)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32099)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32099)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.6261\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t26.87s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.31s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 556.23s of the 852.73s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=2.74%)\n",
            "\u001b[36m(_ray_fit pid=32190)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32190)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32190)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32190)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32190)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32190)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32321)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32321)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32321)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32321)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32321)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32321)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32377)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32377)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32377)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32377)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32377)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32377)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32464)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32464)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32464)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32464)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32464)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32464)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.6726\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t31.1s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.77s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 520.05s of the 816.56s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.5599\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t1.57s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.16s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 518.25s of the 814.76s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.471\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.93s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.16s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 517.1s of the 813.61s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=4.83%)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.6298\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t99.68s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.71s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 414.03s of the 710.54s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_ray_fit pid=32487)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=32487)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=32487)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32487)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=32487)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=32487)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.5102\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t1.44s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.16s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 412.37s of the 708.88s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.5582\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.81s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.17s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 411.33s of the 707.83s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.40%)\n",
            "\u001b[36m(_ray_fit pid=33269)\u001b[0m /usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:182: RuntimeWarning: invalid value encountered in divide\n",
            "\u001b[36m(_ray_fit pid=33269)\u001b[0m   (X[self.cont_columns].values - cont_mean) / cont_std,\n",
            "\u001b[36m(_ray_fit pid=33269)\u001b[0m /usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:182: RuntimeWarning: divide by zero encountered in divide\n",
            "\u001b[36m(_ray_fit pid=33269)\u001b[0m   (X[self.cont_columns].values - cont_mean) / cont_std,\n",
            "\u001b[36m(_ray_fit pid=33269)\u001b[0m /usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:182: RuntimeWarning: invalid value encountered in divide\n",
            "\u001b[36m(_ray_fit pid=33269)\u001b[0m   (X[self.cont_columns].values - cont_mean) / cont_std,\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training... Skipping this model.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=33269, ip=172.28.0.12)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     try: self(f'before_{event_type}');  f()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self(f'after_{event_type}');  final()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return list(res)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return self.func(*fargs, **kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     try: res = getcallable(self, event_name)()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tWARNING: NaN loss encountered in epoch 0!\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Detailed Traceback:\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._fit_folds(\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     raise processed_exception\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return fn(*args, **kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return func(*args, **kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     raise value.as_instanceof_cause()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=33269, ip=172.28.0.12)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     try: self(f'before_{event_type}');  f()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self(f'after_{event_type}');  final()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return list(res)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return self.func(*fargs, **kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     try: res = getcallable(self, event_name)()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tWARNING: NaN loss encountered in epoch 0!\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 397.28s of the 693.79s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=5.26%)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.6058\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t22.12s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.46s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 372.37s of the 668.87s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.5726\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t69.67s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t3.56s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 298.59s of the 595.09s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=9.90%)\n",
            "\u001b[36m(_ray_fit pid=34257)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=34257)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=34257)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=34257)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=34257)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=34257)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=34257)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=34357)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34357)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34357)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34357)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34357)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34357)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34444)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34444)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34444)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34444)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34444)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34444)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34544)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34544)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34544)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34544)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34544)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34544)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.5957\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t33.37s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.37s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 258.35s of the 554.84s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_ray_fit pid=34549)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=34549)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=34549)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34549)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=34549)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=34549)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=5.06%)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.5829\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t61.44s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.56s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 189.76s of the 486.26s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.7428\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t74.48s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t4.56s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 111.54s of the 408.04s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=4.21%)\n",
            "\u001b[36m(_ray_fit pid=35713)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=35713)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=35713)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=35713)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=35713)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=35713)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=35713)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=35817)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=35817)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=35817)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=35817)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=35817)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=35817)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=35921)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=35921)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=35921)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=35921)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=35921)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=35921)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36006)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36006)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36006)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36006)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36006)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36006)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.6916\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t37.62s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.32s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 68.07s of the 364.57s of remaining time.\n",
            "\u001b[36m(_ray_fit pid=36034)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=36034)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=36034)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36034)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=36034)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=36034)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.41%)\n",
            "\u001b[36m(_ray_fit pid=36121)\u001b[0m /usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:182: RuntimeWarning: invalid value encountered in divide\n",
            "\u001b[36m(_ray_fit pid=36121)\u001b[0m   (X[self.cont_columns].values - cont_mean) / cont_std,\n",
            "\u001b[36m(_ray_fit pid=36121)\u001b[0m /usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:182: RuntimeWarning: divide by zero encountered in divide\n",
            "\u001b[36m(_ray_fit pid=36121)\u001b[0m   (X[self.cont_columns].values - cont_mean) / cont_std,\n",
            "\u001b[36m(_ray_fit pid=36121)\u001b[0m /usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:182: RuntimeWarning: invalid value encountered in divide\n",
            "\u001b[36m(_ray_fit pid=36121)\u001b[0m   (X[self.cont_columns].values - cont_mean) / cont_std,\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tWarning: Exception caused NeuralNetFastAI_r191_BAG_L1 to fail during training... Skipping this model.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=36121, ip=172.28.0.12)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     try: self(f'before_{event_type}');  f()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self(f'after_{event_type}');  final()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return list(res)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return self.func(*fargs, **kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     try: res = getcallable(self, event_name)()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tWARNING: NaN loss encountered in epoch 0!\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Detailed Traceback:\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._fit_folds(\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     raise processed_exception\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return fn(*args, **kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return func(*args, **kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     raise value.as_instanceof_cause()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=36121, ip=172.28.0.12)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     try: self(f'before_{event_type}');  f()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self(f'after_{event_type}');  final()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return list(res)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return self.func(*fargs, **kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     try: res = getcallable(self, event_name)()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tWARNING: NaN loss encountered in epoch 0!\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 55.91s of the 352.41s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.06% memory usage per fold, 40.12%/80.00% total).\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=20.06%)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.6165\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t57.95s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t1.08s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 287.57s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tEnsemble Weights: {'CatBoost_r177_BAG_L1': 0.25, 'KNeighborsUnif_BAG_L1': 0.167, 'KNeighborsDist_BAG_L1': 0.167, 'XGBoost_BAG_L1': 0.167, 'NeuralNetTorch_r79_BAG_L1': 0.167, 'CatBoost_BAG_L1': 0.083}\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.796\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.67s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.01s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting 108 L2 models ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 286.84s of the 286.52s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=2.41%)\n",
            "\u001b[36m(_ray_fit pid=36770)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=36770)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=36770)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=36770)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=36770)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=36770)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=36770)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=36859)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36859)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36859)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36859)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36859)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36859)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36937)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36937)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36937)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36937)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36937)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=36937)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37038)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37038)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37038)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37038)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37038)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37038)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.6391\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t26.77s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.33s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 254.5s of the 254.2s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=2.67%)\n",
            "\u001b[36m(_ray_fit pid=37128)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37128)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37128)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37128)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37128)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37128)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37224)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37224)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37224)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37224)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37224)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37224)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37308)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37308)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37308)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37308)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37308)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37308)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37400)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37400)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37400)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37400)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37400)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37400)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.7582\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t31.24s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.31s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 218.34s of the 218.06s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.5905\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t1.43s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.18s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 216.66s of the 216.38s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.6147\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.89s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.15s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 215.55s of the 215.27s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=4.72%)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.7473\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t136.73s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.6s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 75.66s of the 75.37s of remaining time.\n",
            "\u001b[36m(_ray_fit pid=37401)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=37401)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=37401)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37401)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=37401)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=37401)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.5592\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t1.03s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.16s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 74.33s of the 74.05s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.6024\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.79s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.15s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 73.32s of the 73.04s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.40%)\n",
            "\u001b[36m(_ray_fit pid=38346)\u001b[0m /usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:182: RuntimeWarning: invalid value encountered in divide\n",
            "\u001b[36m(_ray_fit pid=38346)\u001b[0m   (X[self.cont_columns].values - cont_mean) / cont_std,\n",
            "\u001b[36m(_ray_fit pid=38346)\u001b[0m /usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:182: RuntimeWarning: divide by zero encountered in divide\n",
            "\u001b[36m(_ray_fit pid=38346)\u001b[0m   (X[self.cont_columns].values - cont_mean) / cont_std,\n",
            "\u001b[36m(_ray_fit pid=38346)\u001b[0m /usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:182: RuntimeWarning: invalid value encountered in divide\n",
            "\u001b[36m(_ray_fit pid=38346)\u001b[0m   (X[self.cont_columns].values - cont_mean) / cont_std,\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training... Skipping this model.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=38346, ip=172.28.0.12)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     try: self(f'before_{event_type}');  f()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self(f'after_{event_type}');  final()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return list(res)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return self.func(*fargs, **kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     try: res = getcallable(self, event_name)()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tWARNING: NaN loss encountered in epoch 0!\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Detailed Traceback:\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._fit_folds(\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     raise processed_exception\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return fn(*args, **kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return func(*args, **kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     raise value.as_instanceof_cause()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=38346, ip=172.28.0.12)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     try: self(f'before_{event_type}');  f()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     self(f'after_{event_type}');  final()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return list(res)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     return self.func(*fargs, **kwargs)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     try: res = getcallable(self, event_name)()\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m     raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tWARNING: NaN loss encountered in epoch 0!\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 59.61s of the 59.32s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=5.45%)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.6107\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t21.74s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.33s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 34.25s of the 33.96s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.24%)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tTime limit exceeded... Skipping NeuralNetTorch_BAG_L2.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 19.54s of the 19.25s of remaining time.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.66% memory usage per fold, 46.65%/80.00% total).\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=11.66%)\n",
            "\u001b[36m(_ray_fit pid=38915)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=38915)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=38915)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=38915)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=38915)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=38915)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=38915)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=39017)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39017)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39017)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39017)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39017)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39017)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39126)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39126)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39126)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39126)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39126)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39126)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39217)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39217)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39217)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39217)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39217)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39217)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.7127\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t39.97s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.64s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -27.2s of remaining time.\n",
            "\u001b[36m(_ray_fit pid=39245)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=39245)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=39245)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39245)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=39245)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=39245)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L2': 0.333, 'LightGBMLarge_BAG_L2': 0.25, 'CatBoost_BAG_L2': 0.167, 'KNeighborsDist_BAG_L1': 0.083, 'NeuralNetTorch_r79_BAG_L1': 0.083, 'CatBoost_r9_BAG_L1': 0.083}\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.889\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.19s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.0s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m AutoGluon training complete, total runtime = 920.09s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 3.9 rows/s (56 batch size)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.08s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.13s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: KNeighborsDist_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.06s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.07s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t1.06s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: LightGBM_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.27s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t1.57s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.16s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.93s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.16s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: CatBoost_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t1.27s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t1.44s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.16s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.81s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.17s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: XGBoost_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.29s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t3.66s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.53s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: CatBoost_r177_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t1.15s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t2.9s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: LightGBM_r131_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.36s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: CatBoost_r9_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t3.69s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tEnsemble Weights: {'CatBoost_r177_BAG_L1': 0.25, 'KNeighborsUnif_BAG_L1': 0.167, 'KNeighborsDist_BAG_L1': 0.167, 'XGBoost_BAG_L1': 0.167, 'NeuralNetTorch_r79_BAG_L1': 0.167, 'CatBoost_BAG_L1': 0.083}\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.67s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting 1 L2 models ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.34s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting 1 L2 models ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: LightGBM_BAG_L2_FULL ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.35s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: RandomForestGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t1.43s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.18s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: RandomForestEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.89s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.15s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting 1 L2 models ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: CatBoost_BAG_L2_FULL ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t3.83s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: ExtraTreesGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t1.03s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.16s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.79s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.15s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting 1 L2 models ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: XGBoost_BAG_L2_FULL ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.21s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting 1 L2 models ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: LightGBMLarge_BAG_L2_FULL ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.37s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L2': 0.333, 'LightGBMLarge_BAG_L2': 0.25, 'CatBoost_BAG_L2': 0.167, 'KNeighborsDist_BAG_L1': 0.083, 'NeuralNetTorch_r79_BAG_L1': 0.083, 'CatBoost_r9_BAG_L1': 0.083}\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m \t0.19s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Refit complete, total runtime = 22.0s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/ieee-fraud-detection/auto-gluon-model/ds_sub_fit/sub_fit_ho\")\n",
            "\u001b[36m(_dystack pid=31649)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                             model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0           LightGBMXT_BAG_L2_FULL       0.879630   0.639122     roc_auc        1.533617            NaN  20.407639                 0.025824                     NaN           0.338164            2       True         18\n",
            "1              XGBoost_BAG_L2_FULL       0.768519   0.610744     roc_auc        1.543401            NaN  20.277120                 0.035608                     NaN           0.207644            2       True         25\n",
            "2           LightGBMXT_BAG_L1_FULL       0.750000   0.626093     roc_auc        0.030280            NaN   1.061241                 0.030280                     NaN           1.061241            1       True          3\n",
            "3        LightGBMLarge_BAG_L2_FULL       0.740741   0.712654     roc_auc        1.532407            NaN  20.443988                 0.024614                     NaN           0.374513            2       True         26\n",
            "4             LightGBM_BAG_L2_FULL       0.722222   0.758165     roc_auc        1.530077            NaN  20.424457                 0.022285                     NaN           0.354982            2       True         19\n",
            "5       NeuralNetTorch_BAG_L1_FULL       0.712963   0.572550     roc_auc        0.281539            NaN   3.657453                 0.281539                     NaN           3.657453            1       True         11\n",
            "6     RandomForestGini_BAG_L2_FULL       0.662037   0.590487     roc_auc        1.612788            NaN  21.503236                 0.104995                0.179111           1.433760            2       True         20\n",
            "7       ExtraTreesEntr_BAG_L1_FULL       0.643519   0.558183     roc_auc        0.141104       0.167512   0.814873                 0.141104                0.167512           0.814873            1       True          9\n",
            "8         WeightedEnsemble_L3_FULL       0.638889   0.888988     roc_auc        1.619348            NaN  24.822391                 0.007744                     NaN           0.190799            3       True         27\n",
            "9       ExtraTreesGini_BAG_L2_FULL       0.625000   0.559165     roc_auc        1.596338            NaN  21.097830                 0.088545                0.161430           1.028354            2       True         23\n",
            "10      ExtraTreesEntr_BAG_L2_FULL       0.592593   0.602445     roc_auc        1.591569            NaN  20.857517                 0.083776                0.150599           0.788042            2       True         24\n",
            "11    RandomForestEntr_BAG_L2_FULL       0.574074   0.614671     roc_auc        1.591729            NaN  20.956514                 0.083936                0.148888           0.887039            2       True         21\n",
            "12            CatBoost_BAG_L2_FULL       0.574074   0.747278     roc_auc        1.564706            NaN  23.902097                 0.056913                     NaN           3.832622            2       True         22\n",
            "13    RandomForestGini_BAG_L1_FULL       0.569444   0.559879     roc_auc        0.102475       0.163914   1.566965                 0.102475                0.163914           1.566965            1       True          5\n",
            "14      ExtraTreesGini_BAG_L1_FULL       0.550926   0.510173     roc_auc        0.090559       0.162391   1.442375                 0.090559                0.162391           1.442375            1       True          8\n",
            "15       LightGBM_r131_BAG_L1_FULL       0.500000   0.691594     roc_auc        0.029379            NaN   0.360544                 0.029379                     NaN           0.360544            1       True         15\n",
            "16         CatBoost_r9_BAG_L1_FULL       0.490741   0.616455     roc_auc        0.076166            NaN   3.691164                 0.076166                     NaN           3.691164            1       True         16\n",
            "17            CatBoost_BAG_L1_FULL       0.425926   0.629841     roc_auc        0.080163            NaN   1.267850                 0.080163                     NaN           1.267850            1       True          7\n",
            "18            LightGBM_BAG_L1_FULL       0.421296   0.672586     roc_auc        0.029114            NaN   0.267636                 0.029114                     NaN           0.267636            1       True          4\n",
            "19      KNeighborsDist_BAG_L1_FULL       0.407407   0.595306     roc_auc        0.013432       0.072664   0.063493                 0.013432                0.072664           0.063493            1       True          2\n",
            "20      KNeighborsUnif_BAG_L1_FULL       0.407407   0.587810     roc_auc        0.014941       0.132090   0.077090                 0.014941                0.132090           0.077090            1       True          1\n",
            "21       CatBoost_r177_BAG_L1_FULL       0.407407   0.582902     roc_auc        0.069886            NaN   1.153194                 0.069886                     NaN           1.153194            1       True         13\n",
            "22       LightGBMLarge_BAG_L1_FULL       0.388889   0.595663     roc_auc        0.023232            NaN   0.525031                 0.023232                     NaN           0.525031            1       True         12\n",
            "23             XGBoost_BAG_L1_FULL       0.351852   0.605836     roc_auc        0.063983            NaN   0.290723                 0.063983                     NaN           0.290723            1       True         10\n",
            "24    RandomForestEntr_BAG_L1_FULL       0.337963   0.470998     roc_auc        0.092683       0.157125   0.930066                 0.092683                0.157125           0.930066            1       True          6\n",
            "25        WeightedEnsemble_L2_FULL       0.287037   0.796002     roc_auc        0.620546            NaN   6.426553                 0.009283                     NaN           0.674428            2       True         17\n",
            "26  NeuralNetTorch_r79_BAG_L1_FULL       0.222222   0.742816     roc_auc        0.368858            NaN   2.899775                 0.368858                     NaN           2.899775            1       True         14\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t955s\t = DyStack   runtime |\t2645s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 2645s\n",
            "AutoGluon will save models to \"/content/ieee-fraud-detection/auto-gluon-model\"\n",
            "Train Data Rows:    500\n",
            "Train Data Columns: 433\n",
            "Label Column:       isFraud\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    5037.72 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.18 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 20 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 8): ['V107', 'V111', 'V113', 'V117', 'V118', 'V119', 'V120', 'V305']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tUnused Original Features (Count: 14): ['V16', 'V18', 'V22', 'V28', 'V32', 'V157', 'V196', 'V199', 'V237', 'V241', 'V244', 'V252', 'V278', 'id_27']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', [])  : 13 | ['V16', 'V18', 'V22', 'V28', 'V32', ...]\n",
            "\t\t('object', []) :  1 | ['id_27']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 378 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t('int', [])    :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t\t('object', []) :  30 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  29 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t('float', [])     : 361 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t('int', [])       :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t\t('int', ['bool']) :  18 | ['C3', 'M1', 'V1', 'V14', 'V41', ...]\n",
            "\t6.6s = Fit runtime\n",
            "\t411 features in original data used to generate 411 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.42 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 6.77s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
            "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 110 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1758.21s of the 2637.92s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.5363\t = Validation score   (roc_auc)\n",
            "\t0.04s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1755.84s of the 2635.55s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.5399\t = Validation score   (roc_auc)\n",
            "\t0.04s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1755.67s of the 2635.38s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.52%)\n",
            "\t0.6267\t = Validation score   (roc_auc)\n",
            "\t31.76s\t = Training   runtime\n",
            "\t0.29s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1720.53s of the 2600.24s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.58%)\n",
            "\t0.5759\t = Validation score   (roc_auc)\n",
            "\t28.45s\t = Training   runtime\n",
            "\t0.24s\t = Validation runtime\n",
            "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 1686.83s of the 2566.54s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.5474\t = Validation score   (roc_auc)\n",
            "\t1.65s\t = Training   runtime\n",
            "\t0.3s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 1684.75s of the 2564.47s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.4982\t = Validation score   (roc_auc)\n",
            "\t1.58s\t = Training   runtime\n",
            "\t0.35s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ... Training model for up to 1682.67s of the 2562.39s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=3.17%)\n",
            "\t0.6331\t = Validation score   (roc_auc)\n",
            "\t107.38s\t = Training   runtime\n",
            "\t0.69s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 1568.25s of the 2447.96s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.507\t = Validation score   (roc_auc)\n",
            "\t1.01s\t = Training   runtime\n",
            "\t0.21s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 1566.94s of the 2446.65s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.58\t = Validation score   (roc_auc)\n",
            "\t0.88s\t = Training   runtime\n",
            "\t0.17s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1565.8s of the 2445.52s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.29%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=41101, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=41101, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1550.71s of the 2430.42s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=3.35%)\n",
            "2024-10-06 05:34:22,156\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t0.5671\t = Validation score   (roc_auc)\n",
            "\t23.07s\t = Training   runtime\n",
            "\t0.35s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1524.05s of the 2403.76s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.16%)\n",
            "\t0.5607\t = Validation score   (roc_auc)\n",
            "\t73.06s\t = Training   runtime\n",
            "\t2.52s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1445.91s of the 2325.63s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=6.18%)\n",
            "\t0.5246\t = Validation score   (roc_auc)\n",
            "\t39.08s\t = Training   runtime\n",
            "\t0.3s\t = Validation runtime\n",
            "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 1402.93s of the 2282.65s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=3.18%)\n",
            "\t0.58\t = Validation score   (roc_auc)\n",
            "\t69.31s\t = Training   runtime\n",
            "\t0.74s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 1327.54s of the 2207.26s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.17%)\n",
            "\t0.7115\t = Validation score   (roc_auc)\n",
            "\t79.78s\t = Training   runtime\n",
            "\t4.67s\t = Validation runtime\n",
            "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 1243.03s of the 2122.74s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=2.67%)\n",
            "\t0.5675\t = Validation score   (roc_auc)\n",
            "\t31.67s\t = Training   runtime\n",
            "\t0.26s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 1207.43s of the 2087.13s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.31%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=44060, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=44060, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 1194.58s of the 2074.29s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.60% memory usage per fold, 50.39%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=12.60%)\n",
            "2024-10-06 05:40:18,438\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t0.6773\t = Validation score   (roc_auc)\n",
            "\t174.05s\t = Training   runtime\n",
            "\t0.74s\t = Validation runtime\n",
            "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 1016.22s of the 1895.93s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.93%)\n",
            "\t0.5824\t = Validation score   (roc_auc)\n",
            "\t36.99s\t = Training   runtime\n",
            "\t0.35s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 975.24s of the 1854.95s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.17%)\n",
            "\t0.5792\t = Validation score   (roc_auc)\n",
            "\t76.58s\t = Training   runtime\n",
            "\t3.22s\t = Validation runtime\n",
            "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 894.43s of the 1774.15s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.48% memory usage per fold, 48.97%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=24.48%)\n",
            "\t0.6336\t = Validation score   (roc_auc)\n",
            "\t27.71s\t = Training   runtime\n",
            "\t0.31s\t = Validation runtime\n",
            "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 862.97s of the 1742.68s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.563\t = Validation score   (roc_auc)\n",
            "\t3.16s\t = Training   runtime\n",
            "\t0.26s\t = Validation runtime\n",
            "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 859.4s of the 1739.11s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.90%)\n",
            "\t0.659\t = Validation score   (roc_auc)\n",
            "\t47.65s\t = Training   runtime\n",
            "\t0.51s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 806.55s of the 1686.27s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.31%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_r102_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=46965, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=46965, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 791.79s of the 1671.5s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.29% memory usage per fold, 49.14%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=12.29%)\n",
            "2024-10-06 05:46:59,767\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t0.6322\t = Validation score   (roc_auc)\n",
            "\t230.91s\t = Training   runtime\n",
            "\t0.62s\t = Validation runtime\n",
            "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 557.58s of the 1437.29s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.5137\t = Validation score   (roc_auc)\n",
            "\t5.18s\t = Training   runtime\n",
            "\t0.16s\t = Validation runtime\n",
            "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 552.08s of the 1431.79s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=5.82%)\n",
            "\t0.6667\t = Validation score   (roc_auc)\n",
            "\t36.78s\t = Training   runtime\n",
            "\t0.28s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 511.65s of the 1391.36s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.31%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_r145_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=48710, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=48710, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 500.16s of the 1379.88s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=3.34%)\n",
            "2024-10-06 05:51:51,967\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t0.6045\t = Validation score   (roc_auc)\n",
            "\t23.55s\t = Training   runtime\n",
            "\t0.44s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 469.16s of the 1348.88s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.17%)\n",
            "\t0.6821\t = Validation score   (roc_auc)\n",
            "\t84.51s\t = Training   runtime\n",
            "\t3.38s\t = Validation runtime\n",
            "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 380.37s of the 1260.08s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=4.43%)\n",
            "\t0.5889\t = Validation score   (roc_auc)\n",
            "\t31.86s\t = Training   runtime\n",
            "\t0.23s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 340.2s of the 1219.92s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.17%)\n",
            "\t0.5467\t = Validation score   (roc_auc)\n",
            "\t79.55s\t = Training   runtime\n",
            "\t3.42s\t = Validation runtime\n",
            "Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 254.99s of the 1134.7s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.94%)\n",
            "\t0.6345\t = Validation score   (roc_auc)\n",
            "\t65.31s\t = Training   runtime\n",
            "\t0.74s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 183.47s of the 1063.19s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.31%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_r11_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=51389, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=51389, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 172.19s of the 1051.9s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=6.37%)\n",
            "2024-10-06 05:57:19,225\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t0.5514\t = Validation score   (roc_auc)\n",
            "\t31.08s\t = Training   runtime\n",
            "\t0.93s\t = Validation runtime\n",
            "Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 134.68s of the 1014.4s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.5807\t = Validation score   (roc_auc)\n",
            "\t1.94s\t = Training   runtime\n",
            "\t0.28s\t = Validation runtime\n",
            "Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 132.34s of the 1012.05s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.59%)\n",
            "\t0.5902\t = Validation score   (roc_auc)\n",
            "\t75.74s\t = Training   runtime\n",
            "\t0.55s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 51.63s of the 931.34s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.31%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_r103_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=52447, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=52447, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Fitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 36.08s of the 915.8s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.17%)\n",
            "2024-10-06 05:59:35,325\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\tTime limit exceeded... Skipping NeuralNetTorch_r14_BAG_L1.\n",
            "2024-10-06 05:59:43,301\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 23.15s of the 902.86s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.17% memory usage per fold, 48.68%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=12.17%)\n",
            "\t0.5746\t = Validation score   (roc_auc)\n",
            "\t32.81s\t = Training   runtime\n",
            "\t0.39s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 863.44s of remaining time.\n",
            "\tEnsemble Weights: {'NeuralNetTorch_r30_BAG_L1': 0.667, 'NeuralNetTorch_r79_BAG_L1': 0.25, 'CatBoost_r137_BAG_L1': 0.083}\n",
            "\t0.8447\t = Validation score   (roc_auc)\n",
            "\t0.31s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 108 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 863.08s of the 862.9s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.80%)\n",
            "\t0.732\t = Validation score   (roc_auc)\n",
            "\t28.87s\t = Training   runtime\n",
            "\t0.23s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 827.23s of the 827.06s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.77%)\n",
            "\t0.6554\t = Validation score   (roc_auc)\n",
            "\t32.55s\t = Training   runtime\n",
            "\t0.25s\t = Validation runtime\n",
            "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 787.38s of the 787.18s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.5824\t = Validation score   (roc_auc)\n",
            "\t1.31s\t = Training   runtime\n",
            "\t0.25s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 785.67s of the 785.49s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.5808\t = Validation score   (roc_auc)\n",
            "\t1.73s\t = Training   runtime\n",
            "\t0.27s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L2 ... Training model for up to 783.49s of the 783.3s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=3.24%)\n",
            "\t0.6678\t = Validation score   (roc_auc)\n",
            "\t150.15s\t = Training   runtime\n",
            "\t0.53s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 626.03s of the 625.85s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.6258\t = Validation score   (roc_auc)\n",
            "\t1.36s\t = Training   runtime\n",
            "\t0.25s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 624.26s of the 624.08s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.5731\t = Validation score   (roc_auc)\n",
            "\t1.05s\t = Training   runtime\n",
            "\t0.25s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 622.84s of the 622.64s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.30%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=54940, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=54940, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Fitting model: XGBoost_BAG_L2 ... Training model for up to 582.25s of the 582.06s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=3.54%)\n",
            "2024-10-06 06:05:09,593\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t0.6153\t = Validation score   (roc_auc)\n",
            "\t26.79s\t = Training   runtime\n",
            "\t0.54s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 548.4s of the 548.2s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.18%)\n",
            "\t0.5993\t = Validation score   (roc_auc)\n",
            "\t83.49s\t = Training   runtime\n",
            "\t4.03s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 456.62s of the 456.43s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=7.27%)\n",
            "\t0.7121\t = Validation score   (roc_auc)\n",
            "\t48.01s\t = Training   runtime\n",
            "\t0.31s\t = Validation runtime\n",
            "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 404.04s of the 403.83s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=3.50%)\n",
            "\t0.6793\t = Validation score   (roc_auc)\n",
            "\t98.7s\t = Training   runtime\n",
            "\t0.55s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 296.99s of the 296.79s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.17%)\n",
            "\t0.7663\t = Validation score   (roc_auc)\n",
            "\t83.64s\t = Training   runtime\n",
            "\t3.38s\t = Validation runtime\n",
            "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 207.8s of the 207.6s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=3.13%)\n",
            "\t0.6742\t = Validation score   (roc_auc)\n",
            "\t37.28s\t = Training   runtime\n",
            "\t0.26s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 165.25s of the 165.06s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.32%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=58373, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=58373, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 129.0s of the 128.81s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 13.58% memory usage per fold, 54.34%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=13.58%)\n",
            "2024-10-06 06:12:43,967\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t0.6669\t = Validation score   (roc_auc)\n",
            "\t119.65s\t = Training   runtime\n",
            "\t0.47s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 2.67s of remaining time.\n",
            "\tEnsemble Weights: {'NeuralNetTorch_r79_BAG_L2': 0.45, 'LightGBMXT_BAG_L2': 0.2, 'NeuralNetTorch_r79_BAG_L1': 0.1, 'NeuralNetTorch_r30_BAG_L1': 0.1, 'LightGBM_BAG_L2': 0.05, 'CatBoost_BAG_L2': 0.05, 'NeuralNetTorch_BAG_L2': 0.05}\n",
            "\t0.8957\t = Validation score   (roc_auc)\n",
            "\t0.22s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 2642.38s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1.8 rows/s (63 batch size)\n",
            "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "Fitting model: KNeighborsUnif_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.04s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.04s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "\t1.18s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBM_BAG_L1_FULL ...\n",
            "\t0.34s\t = Training   runtime\n",
            "Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t1.65s\t = Training   runtime\n",
            "\t0.3s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t1.58s\t = Training   runtime\n",
            "\t0.35s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost_BAG_L1_FULL ...\n",
            "\t1.26s\t = Training   runtime\n",
            "Fitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t1.01s\t = Training   runtime\n",
            "\t0.21s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.88s\t = Training   runtime\n",
            "\t0.17s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: XGBoost_BAG_L1_FULL ...\n",
            "\t0.25s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
            "\t2.65s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
            "\t0.51s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost_r177_BAG_L1_FULL ...\n",
            "\t0.95s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: NeuralNetTorch_r79_BAG_L1_FULL ...\n",
            "\t2.69s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBM_r131_BAG_L1_FULL ...\n",
            "\t0.77s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost_r9_BAG_L1_FULL ...\n",
            "\t9.01s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBM_r96_BAG_L1_FULL ...\n",
            "\t0.41s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: NeuralNetTorch_r22_BAG_L1_FULL ...\n",
            "\t1.31s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: XGBoost_r33_BAG_L1_FULL ...\n",
            "\t0.44s\t = Training   runtime\n",
            "Fitting model: ExtraTrees_r42_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t3.16s\t = Training   runtime\n",
            "\t0.26s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost_r137_BAG_L1_FULL ...\n",
            "\t0.88s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost_r13_BAG_L1_FULL ...\n",
            "\t11.15s\t = Training   runtime\n",
            "Fitting model: RandomForest_r195_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t5.18s\t = Training   runtime\n",
            "\t0.16s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBM_r188_BAG_L1_FULL ...\n",
            "\t0.67s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: XGBoost_r89_BAG_L1_FULL ...\n",
            "\t0.3s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: NeuralNetTorch_r30_BAG_L1_FULL ...\n",
            "\t2.16s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBM_r130_BAG_L1_FULL ...\n",
            "\t0.42s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: NeuralNetTorch_r86_BAG_L1_FULL ...\n",
            "\t1.69s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost_r50_BAG_L1_FULL ...\n",
            "\t0.65s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: XGBoost_r194_BAG_L1_FULL ...\n",
            "\t0.28s\t = Training   runtime\n",
            "Fitting model: ExtraTrees_r172_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t1.94s\t = Training   runtime\n",
            "\t0.28s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost_r69_BAG_L1_FULL ...\n",
            "\t1.41s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBM_r161_BAG_L1_FULL ...\n",
            "\t0.5s\t = Training   runtime\n",
            "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'NeuralNetTorch_r30_BAG_L1': 0.667, 'NeuralNetTorch_r79_BAG_L1': 0.25, 'CatBoost_r137_BAG_L1': 0.083}\n",
            "\t0.31s\t = Training   runtime\n",
            "Fitting 1 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
            "\t0.43s\t = Training   runtime\n",
            "Fitting 1 L2 models ...\n",
            "Fitting model: LightGBM_BAG_L2_FULL ...\n",
            "\t0.46s\t = Training   runtime\n",
            "Fitting model: RandomForestGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
            "\t1.31s\t = Training   runtime\n",
            "\t0.25s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
            "\t1.73s\t = Training   runtime\n",
            "\t0.27s\t = Validation runtime\n",
            "Fitting 1 L2 models ...\n",
            "Fitting model: CatBoost_BAG_L2_FULL ...\n",
            "\t2.27s\t = Training   runtime\n",
            "Fitting model: ExtraTreesGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
            "\t1.36s\t = Training   runtime\n",
            "\t0.25s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
            "\t1.05s\t = Training   runtime\n",
            "\t0.25s\t = Validation runtime\n",
            "Fitting 1 L2 models ...\n",
            "Fitting model: XGBoost_BAG_L2_FULL ...\n",
            "\t0.34s\t = Training   runtime\n",
            "Fitting 1 L2 models ...\n",
            "Fitting model: NeuralNetTorch_BAG_L2_FULL ...\n",
            "\t3.66s\t = Training   runtime\n",
            "Fitting 1 L2 models ...\n",
            "Fitting model: LightGBMLarge_BAG_L2_FULL ...\n",
            "\t0.63s\t = Training   runtime\n",
            "Fitting 1 L2 models ...\n",
            "Fitting model: CatBoost_r177_BAG_L2_FULL ...\n",
            "\t2.64s\t = Training   runtime\n",
            "Fitting 1 L2 models ...\n",
            "Fitting model: NeuralNetTorch_r79_BAG_L2_FULL ...\n",
            "\t2.78s\t = Training   runtime\n",
            "Fitting 1 L2 models ...\n",
            "Fitting model: LightGBM_r131_BAG_L2_FULL ...\n",
            "\t0.44s\t = Training   runtime\n",
            "Fitting 1 L2 models ...\n",
            "Fitting model: CatBoost_r9_BAG_L2_FULL ...\n",
            "\t6.26s\t = Training   runtime\n",
            "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'NeuralNetTorch_r79_BAG_L2': 0.45, 'LightGBMXT_BAG_L2': 0.2, 'NeuralNetTorch_r79_BAG_L1': 0.1, 'NeuralNetTorch_r30_BAG_L1': 0.1, 'LightGBM_BAG_L2': 0.05, 'CatBoost_BAG_L2': 0.05, 'NeuralNetTorch_BAG_L2': 0.05}\n",
            "\t0.22s\t = Training   runtime\n",
            "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
            "Refit complete, total runtime = 67.2s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/ieee-fraud-detection/auto-gluon-model\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = predictor.fit_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJfgNkNWvwLu",
        "outputId": "26fc5f6e-d325-46a0-c17a-08997bfd1c73"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                             model  score_val eval_metric  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0              WeightedEnsemble_L3   0.895670     roc_auc      36.352790  1933.032012                0.000688           0.220749            3      False         49\n",
            "1              WeightedEnsemble_L2   0.844674     roc_auc       8.560995   212.246028                0.001003           0.313127            2      False         34\n",
            "2        NeuralNetTorch_r79_BAG_L2   0.766323     roc_auc      31.312760  1637.746307                3.376046          83.640799            2      False         46\n",
            "3                LightGBMXT_BAG_L2   0.732027     roc_auc      28.171375  1582.971227                0.234661          28.865719            2      False         35\n",
            "4             LightGBMLarge_BAG_L2   0.712096     roc_auc      28.251184  1602.114800                0.314470          48.009292            2      False         44\n",
            "5        NeuralNetTorch_r79_BAG_L1   0.711478     roc_auc       4.669856    79.779742                4.669856          79.779742            1      False         14\n",
            "6        NeuralNetTorch_r30_BAG_L1   0.682062     roc_auc       3.375548    84.507824                3.375548          84.507824            1      False         26\n",
            "7             CatBoost_r177_BAG_L2   0.679313     roc_auc      28.488121  1652.803367                0.551407          98.697858            2      False         45\n",
            "8               CatBoost_r9_BAG_L1   0.677251     roc_auc       0.742737   174.046419                0.742737         174.046419            1      False         16\n",
            "9             LightGBM_r131_BAG_L2   0.674158     roc_auc      28.194447  1591.381469                0.257733          37.275961            2      False         47\n",
            "10                 CatBoost_BAG_L2   0.667766     roc_auc      28.466420  1704.257975                0.529706         150.152467            2      False         39\n",
            "11              CatBoost_r9_BAG_L2   0.666942     roc_auc      28.410322  1673.751291                0.473608         119.645782            2      False         48\n",
            "12            LightGBM_r188_BAG_L1   0.666667     roc_auc       0.275058    36.777484                0.275058          36.777484            1      False         24\n",
            "13            CatBoost_r137_BAG_L1   0.658969     roc_auc       0.514589    47.645335                0.514589          47.645335            1      False         21\n",
            "14                 LightGBM_BAG_L2   0.655395     roc_auc      28.186581  1586.660255                0.249867          32.554746            2      False         36\n",
            "15             CatBoost_r50_BAG_L1   0.634502     roc_auc       0.737637    65.308034                0.737637          65.308034            1      False         29\n",
            "16              XGBoost_r33_BAG_L1   0.633608     roc_auc       0.309258    27.714124                0.309258          27.714124            1      False         19\n",
            "17                 CatBoost_BAG_L1   0.633127     roc_auc       0.689596   107.381459                0.689596         107.381459            1      False          7\n",
            "18             CatBoost_r13_BAG_L1   0.632165     roc_auc       0.618009   230.913731                0.618009         230.913731            1      False         22\n",
            "19               LightGBMXT_BAG_L1   0.626667     roc_auc       0.287326    31.760951                0.287326          31.760951            1      False          3\n",
            "20           ExtraTreesGini_BAG_L2   0.625842     roc_auc      28.185637  1555.464691                0.248924           1.359183            2      False         40\n",
            "21                  XGBoost_BAG_L2   0.615258     roc_auc      28.476715  1580.895314                0.540001          26.789805            2      False         42\n",
            "22              XGBoost_r89_BAG_L1   0.604536     roc_auc       0.444640    23.547503                0.444640          23.547503            1      False         25\n",
            "23           NeuralNetTorch_BAG_L2   0.599313     roc_auc      31.961823  1637.597533                4.025109          83.492024            2      False         43\n",
            "24             CatBoost_r69_BAG_L1   0.590241     roc_auc       0.550730    75.737694                0.550730          75.737694            1      False         32\n",
            "25            LightGBM_r130_BAG_L1   0.588866     roc_auc       0.225729    31.862404                0.225729          31.862404            1      False         27\n",
            "26             LightGBM_r96_BAG_L1   0.582405     roc_auc       0.345366    36.991667                0.345366          36.991667            1      False         17\n",
            "27         RandomForestGini_BAG_L2   0.582405     roc_auc      28.187729  1555.418688                0.251015           1.313179            2      False         37\n",
            "28         RandomForestEntr_BAG_L2   0.580756     roc_auc      28.205015  1555.837085                0.268301           1.731576            2      False         38\n",
            "29          ExtraTrees_r172_BAG_L1   0.580687     roc_auc       0.277689     1.940155                0.277689           1.940155            1       True         31\n",
            "30           ExtraTreesEntr_BAG_L1   0.580000     roc_auc       0.165416     0.880367                0.165416           0.880367            1       True          9\n",
            "31            CatBoost_r177_BAG_L1   0.580000     roc_auc       0.737997    69.306768                0.737997          69.306768            1      False         13\n",
            "32       NeuralNetTorch_r22_BAG_L1   0.579244     roc_auc       3.223696    76.576092                3.223696          76.576092            1      False         18\n",
            "33                 LightGBM_BAG_L1   0.575945     roc_auc       0.237908    28.450495                0.237908          28.450495            1      False          4\n",
            "34            LightGBM_r161_BAG_L1   0.574639     roc_auc       0.389571    32.808286                0.389571          32.808286            1      False         33\n",
            "35           ExtraTreesEntr_BAG_L2   0.573127     roc_auc      28.187697  1555.156987                0.250983           1.051479            2      False         41\n",
            "36            LightGBM_r131_BAG_L1   0.567491     roc_auc       0.255796    31.672910                0.255796          31.672910            1      False         15\n",
            "37                  XGBoost_BAG_L1   0.567148     roc_auc       0.348409    23.071923                0.348409          23.071923            1      False         10\n",
            "38           ExtraTrees_r42_BAG_L1   0.562955     roc_auc       0.259427     3.157792                0.259427           3.157792            1       True         20\n",
            "39           NeuralNetTorch_BAG_L1   0.560687     roc_auc       2.522637    73.058343                2.522637          73.058343            1      False         11\n",
            "40             XGBoost_r194_BAG_L1   0.551409     roc_auc       0.931676    31.080595                0.931676          31.080595            1      False         30\n",
            "41         RandomForestGini_BAG_L1   0.547423     roc_auc       0.301051     1.652263                0.301051           1.652263            1       True          5\n",
            "42       NeuralNetTorch_r86_BAG_L1   0.546667     roc_auc       3.416586    79.546184                3.416586          79.546184            1      False         28\n",
            "43           KNeighborsDist_BAG_L1   0.539931     roc_auc       0.009103     0.039787                0.009103           0.039787            1       True          2\n",
            "44           KNeighborsUnif_BAG_L1   0.536289     roc_auc       0.048913     0.043258                0.048913           0.043258            1       True          1\n",
            "45            LightGBMLarge_BAG_L1   0.524605     roc_auc       0.304037    39.079435                0.304037          39.079435            1      False         12\n",
            "46        RandomForest_r195_BAG_L1   0.513677     roc_auc       0.162177     5.184364                0.162177           5.184364            1       True         23\n",
            "47           ExtraTreesGini_BAG_L1   0.507010     roc_auc       0.211524     1.006259                0.211524           1.006259            1       True          8\n",
            "48         RandomForestEntr_BAG_L1   0.498213     roc_auc       0.347023     1.575858                0.347023           1.575858            1       True          6\n",
            "49      KNeighborsDist_BAG_L1_FULL        NaN     roc_auc       0.009103     0.039787                0.009103           0.039787            1       True         51\n",
            "50      KNeighborsUnif_BAG_L1_FULL        NaN     roc_auc       0.048913     0.043258                0.048913           0.043258            1       True         50\n",
            "51   RandomForest_r195_BAG_L1_FULL        NaN     roc_auc       0.162177     5.184364                0.162177           5.184364            1       True         72\n",
            "52      ExtraTreesEntr_BAG_L1_FULL        NaN     roc_auc       0.165416     0.880367                0.165416           0.880367            1       True         58\n",
            "53      ExtraTreesGini_BAG_L1_FULL        NaN     roc_auc       0.211524     1.006259                0.211524           1.006259            1       True         57\n",
            "54      ExtraTrees_r42_BAG_L1_FULL        NaN     roc_auc       0.259427     3.157792                0.259427           3.157792            1       True         69\n",
            "55     ExtraTrees_r172_BAG_L1_FULL        NaN     roc_auc       0.277689     1.940155                0.277689           1.940155            1       True         80\n",
            "56    RandomForestGini_BAG_L1_FULL        NaN     roc_auc       0.301051     1.652263                0.301051           1.652263            1       True         54\n",
            "57    RandomForestEntr_BAG_L1_FULL        NaN     roc_auc       0.347023     1.575858                0.347023           1.575858            1       True         55\n",
            "58         XGBoost_r89_BAG_L1_FULL        NaN     roc_auc            NaN     0.298680                     NaN           0.298680            1       True         74\n",
            "59         XGBoost_r33_BAG_L1_FULL        NaN     roc_auc            NaN     0.435112                     NaN           0.435112            1       True         68\n",
            "60        XGBoost_r194_BAG_L1_FULL        NaN     roc_auc            NaN     0.276220                     NaN           0.276220            1       True         79\n",
            "61             XGBoost_BAG_L2_FULL        NaN     roc_auc            NaN    57.679259                     NaN           0.335559            2       True         91\n",
            "62             XGBoost_BAG_L1_FULL        NaN     roc_auc            NaN     0.248568                     NaN           0.248568            1       True         59\n",
            "63        WeightedEnsemble_L3_FULL        NaN     roc_auc            NaN    67.165283                     NaN           0.220749            3       True         98\n",
            "64        WeightedEnsemble_L2_FULL        NaN     roc_auc            NaN     6.041507                     NaN           0.313127            2       True         83\n",
            "65    RandomForestGini_BAG_L2_FULL        NaN     roc_auc            NaN    58.656879                0.251015           1.313179            2       True         86\n",
            "66    RandomForestEntr_BAG_L2_FULL        NaN     roc_auc            NaN    59.075276                0.268301           1.731576            2       True         87\n",
            "67  NeuralNetTorch_r86_BAG_L1_FULL        NaN     roc_auc            NaN     1.687211                     NaN           1.687211            1       True         77\n",
            "68  NeuralNetTorch_r79_BAG_L2_FULL        NaN     roc_auc            NaN    60.119880                     NaN           2.776181            2       True         95\n",
            "69  NeuralNetTorch_r79_BAG_L1_FULL        NaN     roc_auc            NaN     2.686930                     NaN           2.686930            1       True         63\n",
            "70  NeuralNetTorch_r30_BAG_L1_FULL        NaN     roc_auc            NaN     2.159414                     NaN           2.159414            1       True         75\n",
            "71  NeuralNetTorch_r22_BAG_L1_FULL        NaN     roc_auc            NaN     1.306371                     NaN           1.306371            1       True         67\n",
            "72      NeuralNetTorch_BAG_L2_FULL        NaN     roc_auc            NaN    61.003055                     NaN           3.659356            2       True         92\n",
            "73      NeuralNetTorch_BAG_L1_FULL        NaN     roc_auc            NaN     2.645041                     NaN           2.645041            1       True         60\n",
            "74        LightGBM_r96_BAG_L1_FULL        NaN     roc_auc            NaN     0.413641                     NaN           0.413641            1       True         66\n",
            "75       LightGBM_r188_BAG_L1_FULL        NaN     roc_auc            NaN     0.666547                     NaN           0.666547            1       True         73\n",
            "76       LightGBM_r161_BAG_L1_FULL        NaN     roc_auc            NaN     0.502726                     NaN           0.502726            1       True         82\n",
            "77       LightGBM_r131_BAG_L2_FULL        NaN     roc_auc            NaN    57.786468                     NaN           0.442768            2       True         96\n",
            "78       LightGBM_r131_BAG_L1_FULL        NaN     roc_auc            NaN     0.772432                     NaN           0.772432            1       True         64\n",
            "79       LightGBM_r130_BAG_L1_FULL        NaN     roc_auc            NaN     0.417667                     NaN           0.417667            1       True         76\n",
            "80            LightGBM_BAG_L2_FULL        NaN     roc_auc            NaN    57.807035                     NaN           0.463336            2       True         85\n",
            "81            LightGBM_BAG_L1_FULL        NaN     roc_auc            NaN     0.342906                     NaN           0.342906            1       True         53\n",
            "82          LightGBMXT_BAG_L2_FULL        NaN     roc_auc            NaN    57.771196                     NaN           0.427497            2       True         84\n",
            "83          LightGBMXT_BAG_L1_FULL        NaN     roc_auc            NaN     1.179381                     NaN           1.179381            1       True         52\n",
            "84       LightGBMLarge_BAG_L2_FULL        NaN     roc_auc            NaN    57.970835                     NaN           0.627136            2       True         93\n",
            "85       LightGBMLarge_BAG_L1_FULL        NaN     roc_auc            NaN     0.505837                     NaN           0.505837            1       True         61\n",
            "86      ExtraTreesGini_BAG_L2_FULL        NaN     roc_auc            NaN    58.702882                0.248924           1.359183            2       True         89\n",
            "87      ExtraTreesEntr_BAG_L2_FULL        NaN     roc_auc            NaN    58.395178                0.250983           1.051479            2       True         90\n",
            "88         CatBoost_r9_BAG_L2_FULL        NaN     roc_auc            NaN    63.602920                     NaN           6.259220            2       True         97\n",
            "89         CatBoost_r9_BAG_L1_FULL        NaN     roc_auc            NaN     9.010004                     NaN           9.010004            1       True         65\n",
            "90        CatBoost_r69_BAG_L1_FULL        NaN     roc_auc            NaN     1.409789                     NaN           1.409789            1       True         81\n",
            "91        CatBoost_r50_BAG_L1_FULL        NaN     roc_auc            NaN     0.652734                     NaN           0.652734            1       True         78\n",
            "92       CatBoost_r177_BAG_L2_FULL        NaN     roc_auc            NaN    59.982758                     NaN           2.639059            2       True         94\n",
            "93       CatBoost_r177_BAG_L1_FULL        NaN     roc_auc            NaN     0.946086                     NaN           0.946086            1       True         62\n",
            "94        CatBoost_r13_BAG_L1_FULL        NaN     roc_auc            NaN    11.154648                     NaN          11.154648            1       True         71\n",
            "95       CatBoost_r137_BAG_L1_FULL        NaN     roc_auc            NaN     0.882036                     NaN           0.882036            1       True         70\n",
            "96            CatBoost_BAG_L2_FULL        NaN     roc_auc            NaN    59.618165                     NaN           2.274466            2       True         88\n",
            "97            CatBoost_BAG_L1_FULL        NaN     roc_auc            NaN     1.263615                     NaN           1.263615            1       True         56\n",
            "Number of models trained: 98\n",
            "Types of models trained:\n",
            "{'StackerEnsembleModel_CatBoost', 'StackerEnsembleModel_TabularNeuralNetTorch', 'StackerEnsembleModel_KNN', 'StackerEnsembleModel_XGBoost', 'WeightedEnsembleModel', 'StackerEnsembleModel_RF', 'StackerEnsembleModel_XT', 'StackerEnsembleModel_LGB'}\n",
            "Bagging used: True  (with 8 folds)\n",
            "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', [])  :  29 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "('float', [])     : 361 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "('int', [])       :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "('int', ['bool']) :  18 | ['C3', 'M1', 'V1', 'V14', 'V41', ...]\n",
            "Plot summary of models saved to file: /content/ieee-fraud-detection/auto-gluon-modelSummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        }
      ]
    }
  ]
}